{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT-RCNN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2-Z15bFfZ3DO","executionInfo":{"status":"ok","timestamp":1602251819228,"user_tz":-420,"elapsed":224139,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"44f6e0b9-12ba-49a6-c090-cf8818850b3b","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e9ARMMPuaDHf"},"source":["path = \"/content/drive/My Drive/Aspect-based Sentiment Analysis/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1HsOkCtwaEuf"},"source":["import pickle\n","import pandas as pd\n","df = pd.read_csv(path+\"review_train1.csv\")\n","df_dev = pd.read_csv(path+'review_dev1.csv')\n","df_test = pd.read_csv(path+'review_test1.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPFb7__TaFhT"},"source":["data_train = df.review.to_list()\n","label_train = pickle.load(open(path+'VLSP2018/aspect_train.pkl','rb'))\n","data_dev = df_dev.review.tolist()\n","label_dev =  pickle.load(open(path+'VLSP2018/aspect_dev.pkl','rb'))\n","data_test = df_test.review.tolist()\n","label_test = pickle.load(open(path+'VLSP2018/aspect_test.pkl','rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XyMsKW8IaGiF","executionInfo":{"status":"ok","timestamp":1602251881747,"user_tz":-420,"elapsed":22716,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"08fb8c02-a8ab-4142-b127-50986d31107b","colab":{"base_uri":"https://localhost:8080/","height":177}},"source":["!pip install -q keras-bert==0.85.0\n","!pip install -q keras-rectified-adam"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for keras-rectified-adam (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DRLq1BkhaLkU","executionInfo":{"status":"ok","timestamp":1602251881749,"user_tz":-420,"elapsed":22616,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"fb0da926-e396-462c-f815-fc0f49dfbd7c","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MS2cjbtVsHBw","executionInfo":{"status":"ok","timestamp":1602251884882,"user_tz":-420,"elapsed":25616,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"fd947ce5-daa8-49de-fc01-f42da5b42f9d","colab":{"base_uri":"https://localhost:8080/","height":159}},"source":["pip install keras==2.3.1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras==2.3.1 in /tensorflow-1.15.2/python3.6 (2.3.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.5)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"neqqdmmZaMfc","executionInfo":{"status":"ok","timestamp":1602251890128,"user_tz":-420,"elapsed":30563,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"c3509cbd-287a-4c35-802e-92f99aa4eacf","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import codecs\n","import tensorflow as tf\n","import keras\n","import os\n","from keras_radam import RAdam\n","from keras import backend as K\n","from keras_bert import load_trained_model_from_checkpoint\n","import numpy as np"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OFX3F3ZEq1sv","executionInfo":{"status":"ok","timestamp":1602251909603,"user_tz":-420,"elapsed":49893,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"e6fe17c0-4744-4120-8175-699850e5d3bf","colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["!wget -q https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n","!unzip -o multi_cased_L-12_H-768_A-12.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  multi_cased_L-12_H-768_A-12.zip\n","   creating: multi_cased_L-12_H-768_A-12/\n","  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n","  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n","  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n","  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n","  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bTBpFFc7aOHW"},"source":["pretrained_path = 'multi_cased_L-12_H-768_A-12'\n","config_path = os.path.join(pretrained_path, 'bert_config.json')\n","checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n","vocab_path = os.path.join(pretrained_path, 'vocab.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p48y2Ohpp28v"},"source":["SEQ_LEN = 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jf8HW5chpjpi","executionInfo":{"status":"ok","timestamp":1602251939466,"user_tz":-420,"elapsed":79219,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"fb648128-8fb1-484b-d25f-993fcb3df177","colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["model = load_trained_model_from_checkpoint(\n","    config_path,\n","    checkpoint_path,\n","    seq_len=SEQ_LEN,\n","    output_layer_num=4,\n","    trainable=True\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lF8dztdfIUzk"},"source":["import keras_bert\n","from keras.layers import Dense, Input, Flatten,SpatialDropout1D,Bidirectional\n","from keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate, Dropout,GlobalMaxPool1D,Lambda,MaxPool1D\n","from keras.models import Model\n","from keras.layers import Bidirectional,LSTM,GRU"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgwD-Q6lLfEP"},"source":["LR=3e-5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rrW8gNrpXdr"},"source":["class NonMasking(keras.layers.Layer):   \n","    def __init__(self, **kwargs):   \n","        self.supports_masking = True  \n","        super(NonMasking, self).__init__(**kwargs)   \n","  \n","    def build(self, input_shape):   \n","        input_shape = input_shape   \n","  \n","    def compute_mask(self, input, input_mask=None):   \n","        # do not pass the mask to the next layers   \n","        return None   \n","  \n","    def call(self, x, mask=None):   \n","        return x   \n","  \n","    def get_output_shape_for(self, input_shape):   \n","        return input_shape  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEc_uPxEA3QH"},"source":["def bert_rcnn():\n","  inputs = model.inputs\n","  bert_out = NonMasking()(model.outputs)\n","\n","  bert_out = SpatialDropout1D(0.2)(bert_out)\n","\n","  l_embedding = Lambda(lambda x: K.concatenate([K.zeros(shape=(K.shape(x)[0], 1, K.shape(x)[-1])),\n","                                                        x[:, :-1]], axis=1))(bert_out)\n","          \n","  r_embedding = Lambda(lambda x: K.concatenate([K.zeros(shape=(K.shape(x)[0], 1, K.shape(x)[-1])),\n","                                                        x[:, 1:]], axis=1))(bert_out)\n","\n","  forward = LSTM(256, return_sequences=True)(l_embedding) \n","  backward = LSTM(256, return_sequences=True, go_backwards=True)(r_embedding)\n","  backward = Lambda(lambda x: K.reverse(x, axes=1))(backward)\n","\n","  together = [forward, bert_out , backward]\n","\n","  together = Concatenate(axis=2)(together)\n","\n","  semantic = Conv1D(256, kernel_size=1, activation=\"relu\")(together)\n","  sentence_embed = Lambda(lambda x: K.max(x, axis=1))(semantic)\n","\n","  dense_layer = Dense(768, activation='relu')(sentence_embed)\n","  dense_layer = Dense(512, activation='relu')(dense_layer)\n","  dense_layer = Dense(256, activation='relu')(dense_layer)\n","  preds = Dense(12, activation='sigmoid')(dense_layer)\n","  return Model(inputs, preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"riEX_nmbp9-p"},"source":["def bert_base():\n","  inputs = model.inputs\n","  cls = keras_bert.layers.Extract(index=0)(model.output)\n","  dropout = Dropout(0.5)(cls)\n","  dense_layer = Dense(768, activation='relu')(dropout)\n","  dense_layer = Dense(512, activation='relu')(dense_layer)\n","  dense_layer = Dense(256, activation='relu')(dense_layer)\n","  preds = Dense(12, activation='sigmoid')(dense_layer)\n","  return Model(inputs, preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zj9t_QRfOtlN"},"source":["def bert_textcnn():\n","  inputs = model.inputs\n","  bert_out = NonMasking()(model.outputs)\n","\n","  bert_out = SpatialDropout1D(0.5)(bert_out)\n","\n","  filter_lengths = [2, 3, 4, 5]\n","  conv_layers = []\n","  for filter_length in filter_lengths:\n","      conv_layer = Conv1D(filters=256, kernel_size=filter_length, padding='valid',\n","                          strides=1, activation='relu')(bert_out)\n","      maxpooling = MaxPool1D(pool_size=SEQ_LEN - filter_length + 1)(conv_layer)\n","      flatten = Flatten()(maxpooling)\n","      conv_layers.append(flatten)\n","  sentence_embed = Concatenate()(conv_layers)\n","\n","  dense_layer = Dense(768, activation='relu')(sentence_embed)\n","  dense_layer = Dense(512, activation='relu')(dense_layer)\n","  dense_layer = Dense(256, activation='relu')(dense_layer)\n","  preds = Dense(12, activation='sigmoid')(dense_layer)\n","  return Model(inputs, preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D7n7sAGVDTAV"},"source":["def bert_lstm():\n","  inputs = model.inputs\n","  lstm = Bidirectional(LSTM(units=256))(model.output)\n","  dense_layer = Dense(768, activation='relu')(lstm)\n","  dense_layer = Dense(512, activation='relu')(dense_layer)\n","  dense_layer = Dense(256, activation='relu')(dense_layer)\n","  preds = Dense(12, activation='sigmoid')(dense_layer)\n","  return Model(inputs, preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVAFn-yrMVSH"},"source":["token_dict = {}\n","with codecs.open(vocab_path, 'rb','utf-8') as reader:\n","    for line in reader:\n","        token = line.strip()\n","        token_dict[token] = len(token_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFLhfWk-MV_2"},"source":["from keras_bert import Tokenizer\n","tokenizer = Tokenizer(token_dict,cased=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ErLi5E6uMmbX"},"source":["from sklearn.model_selection import train_test_split\n","def load_data(data, sentiments):\n","    global tokenizer\n","    indices = []\n","    for text in data:\n","      ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n","      indices.append(ids)\n","\n","    return [indices, np.zeros_like(indices)], np.array(sentiments)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8EOQE8s0O03N","executionInfo":{"status":"ok","timestamp":1602252136200,"user_tz":-420,"elapsed":1636,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"4eb65c27-dc85-4051-f9cb-9acd7ad152b7","colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["sess = K.get_session()\n","uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n","init_op = tf.variables_initializer(\n","    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n",")\n","sess.run(init_op)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pu3K59IUMX88"},"source":["X_train,Y_train = load_data(data_train,label_train)\n","X_dev,Y_dev = load_data(data_dev,label_dev)\n","X_test,Y_test = load_data(data_test,label_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEgtCfaK0Kzt"},"source":["def calculating_class_weights(y_true):\n","    from sklearn.utils.class_weight import compute_class_weight\n","    number_dim = np.shape(y_true)[1]\n","    weights = np.empty([number_dim, 2])\n","    for i in range(number_dim):\n","        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n","    return weights\n","cw = calculating_class_weights(Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUW7-hXluMri"},"source":["\n","def f1(y_true, y_pred):\n","    def recall(y_true, y_pred):\n","        \"\"\"Recall metric.\n","\n","        Only computes a batch-wise average of recall.\n","\n","        Computes the recall, a metric for multi-label classification of\n","        how many relevant items are selected.\n","        \"\"\"\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + K.epsilon())\n","        return recall\n","\n","    def precision(y_true, y_pred):\n","        \"\"\"Precision metric.\n","\n","        Only computes a batch-wise average of precision.\n","\n","        Computes the precision, a metric for multi-label classification of\n","        how many selected items are relevant.\n","        \"\"\"\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","    precision = precision(y_true, y_pred)\n","    recall = recall(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTEWtOyq-zne"},"source":["aspect_model = bert_base()\n","#aspect_model = bert_rcnn()\n","#aspect_model = bert_lstm()\n","model.trainable = False\n","aspect_model.compile(loss='binary_crossentropy',\n","              optimizer=RAdam(learning_rate=LR),\n","              metrics=[f1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgzefJuD-1Df"},"source":["from keras.callbacks import ModelCheckpoint \n","checkpoint = ModelCheckpoint(path+'checkpoint2', monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BEJCwOS_MG9N","executionInfo":{"status":"ok","timestamp":1602253694459,"user_tz":-420,"elapsed":425327,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"76813de6-5d01-4df0-d03b-23f857a523a2","colab":{"base_uri":"https://localhost:8080/","height":852}},"source":["aspect_model.fit(X_train,Y_train,epochs=1,batch_size=16,verbose = 1,validation_data=[X_test,Y_test],callbacks=callbacks_list, class_weight=cw)\n","model.trainable = True \n","aspect_model.fit(X_train,Y_train,epochs=10,batch_size=16,verbose = 1,validation_data=[X_test,Y_test],callbacks=callbacks_list, class_weight=cw)       "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 2961 samples, validate on 500 samples\n","Epoch 1/1\n","2961/2961 [==============================] - 128s 43ms/step - loss: 0.5201 - f1: 0.4833 - val_loss: 0.5572 - val_f1: 0.6074\n","\n","Epoch 00001: val_f1 improved from -inf to 0.60744, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Train on 2961 samples, validate on 500 samples\n","Epoch 1/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.3817 - f1: 0.6294 - val_loss: 0.5363 - val_f1: 0.6557\n","\n","Epoch 00001: val_f1 improved from 0.60744 to 0.65572, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Epoch 2/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.3474 - f1: 0.6848 - val_loss: 0.4984 - val_f1: 0.6878\n","\n","Epoch 00002: val_f1 improved from 0.65572 to 0.68784, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Epoch 3/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.3111 - f1: 0.7351 - val_loss: 0.4837 - val_f1: 0.7079\n","\n","Epoch 00003: val_f1 improved from 0.68784 to 0.70790, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Epoch 4/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.2803 - f1: 0.7760 - val_loss: 0.4593 - val_f1: 0.7410\n","\n","Epoch 00004: val_f1 improved from 0.70790 to 0.74101, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Epoch 5/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.2492 - f1: 0.8048 - val_loss: 0.4409 - val_f1: 0.7531\n","\n","Epoch 00005: val_f1 improved from 0.74101 to 0.75307, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Epoch 6/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.2225 - f1: 0.8253 - val_loss: 0.4351 - val_f1: 0.7539\n","\n","Epoch 00006: val_f1 improved from 0.75307 to 0.75386, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Epoch 7/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.1922 - f1: 0.8476 - val_loss: 0.4297 - val_f1: 0.7570\n","\n","Epoch 00007: val_f1 improved from 0.75386 to 0.75703, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Epoch 8/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.1650 - f1: 0.8697 - val_loss: 0.4907 - val_f1: 0.7511\n","\n","Epoch 00008: val_f1 did not improve from 0.75703\n","Epoch 9/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.1369 - f1: 0.8929 - val_loss: 0.4546 - val_f1: 0.7704\n","\n","Epoch 00009: val_f1 improved from 0.75703 to 0.77044, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Epoch 10/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.1107 - f1: 0.9162 - val_loss: 0.5186 - val_f1: 0.7684\n","\n","Epoch 00010: val_f1 did not improve from 0.77044\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7f3415ac2ac8>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"t-ROILQcs1pp","executionInfo":{"status":"ok","timestamp":1602254852868,"user_tz":-420,"elapsed":1482995,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"54eb6fcd-70f2-4b5a-b8f8-374b2121c4aa","colab":{"base_uri":"https://localhost:8080/","height":764}},"source":["aspect_model.fit(X_train,Y_train,epochs=10,batch_size=16,verbose = 1,validation_data=[X_test,Y_test],callbacks=callbacks_list, class_weight=cw)       "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 2961 samples, validate on 500 samples\n","Epoch 1/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.0996 - f1: 0.9248 - val_loss: 0.5784 - val_f1: 0.7499\n","\n","Epoch 00001: val_f1 did not improve from 0.77044\n","Epoch 2/10\n","2961/2961 [==============================] - 113s 38ms/step - loss: 0.0835 - f1: 0.9377 - val_loss: 0.6163 - val_f1: 0.7513\n","\n","Epoch 00002: val_f1 did not improve from 0.77044\n","Epoch 3/10\n","2961/2961 [==============================] - 113s 38ms/step - loss: 0.0663 - f1: 0.9510 - val_loss: 0.6521 - val_f1: 0.7605\n","\n","Epoch 00003: val_f1 did not improve from 0.77044\n","Epoch 4/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.0640 - f1: 0.9524 - val_loss: 0.6495 - val_f1: 0.7697\n","\n","Epoch 00004: val_f1 did not improve from 0.77044\n","Epoch 5/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.0463 - f1: 0.9673 - val_loss: 0.6634 - val_f1: 0.7678\n","\n","Epoch 00005: val_f1 did not improve from 0.77044\n","Epoch 6/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.0355 - f1: 0.9745 - val_loss: 0.7014 - val_f1: 0.7802\n","\n","Epoch 00006: val_f1 improved from 0.77044 to 0.78018, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Epoch 7/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.0318 - f1: 0.9785 - val_loss: 0.8107 - val_f1: 0.7636\n","\n","Epoch 00007: val_f1 did not improve from 0.78018\n","Epoch 8/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.0262 - f1: 0.9815 - val_loss: 0.7885 - val_f1: 0.7808\n","\n","Epoch 00008: val_f1 improved from 0.78018 to 0.78079, saving model to /content/drive/My Drive/Aspect-based Sentiment Analysis/checkpoint2\n","Epoch 9/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.0222 - f1: 0.9850 - val_loss: 0.8797 - val_f1: 0.7719\n","\n","Epoch 00009: val_f1 did not improve from 0.78079\n","Epoch 10/10\n","2961/2961 [==============================] - 114s 38ms/step - loss: 0.0243 - f1: 0.9829 - val_loss: 0.8911 - val_f1: 0.7708\n","\n","Epoch 00010: val_f1 did not improve from 0.78079\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7f3417a17320>"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"iy8jGevwHLgC"},"source":["aspect_model.load_weights(path+'checkpoint2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wAlNPlBuiHqR","executionInfo":{"status":"ok","timestamp":1602073469962,"user_tz":-420,"elapsed":149308,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"d7f1f8fd-3639-41da-e008-2e40bd41c6fc","colab":{"base_uri":"https://localhost:8080/"}},"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","# bert-rcnn\n","predictions= aspect_model.predict(X_test)\n","thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n","for val in thresholds:\n","    pred=predictions.copy()\n","  \n","    pred[pred>=val]=1\n","    pred[pred<val]=0\n","  \n","    precision = precision_score(Y_test, pred, average='micro')\n","    recall = recall_score(Y_test, pred, average='micro')\n","    f1 = f1_score(Y_test, pred, average='micro')\n","   \n","    print(\"Micro-average quality numbers\")\n","    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Micro-average quality numbers\n","Precision: 0.7209, Recall: 0.8446, F1-measure: 0.7778\n","Micro-average quality numbers\n","Precision: 0.7496, Recall: 0.8206, F1-measure: 0.7835\n","Micro-average quality numbers\n","Precision: 0.7663, Recall: 0.8065, F1-measure: 0.7859\n","Micro-average quality numbers\n","Precision: 0.7784, Recall: 0.7958, F1-measure: 0.7870\n","Micro-average quality numbers\n","Precision: 0.7860, Recall: 0.7821, F1-measure: 0.7841\n","Micro-average quality numbers\n","Precision: 0.7989, Recall: 0.7751, F1-measure: 0.7868\n","Micro-average quality numbers\n","Precision: 0.8085, Recall: 0.7594, F1-measure: 0.7832\n","Micro-average quality numbers\n","Precision: 0.8210, Recall: 0.7453, F1-measure: 0.7814\n","Micro-average quality numbers\n","Precision: 0.8349, Recall: 0.7131, F1-measure: 0.7692\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pckco7HuiKwr","executionInfo":{"status":"ok","timestamp":1600933188216,"user_tz":-420,"elapsed":2539395,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"16c8dd1d-8d40-4546-ab3c-0acd59ee921e","colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","# bert-base\n","predictions= aspect_model.predict(X_test)\n","thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n","for val in thresholds:\n","    pred=predictions.copy()\n","  \n","    pred[pred>=val]=1\n","    pred[pred<val]=0\n","  \n","    precision = precision_score(Y_test, pred, average='micro')\n","    recall = recall_score(Y_test, pred, average='micro')\n","    f1 = f1_score(Y_test, pred, average='micro')\n","   \n","    print(\"Micro-average quality numbers\")\n","    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Micro-average quality numbers\n","Precision: 0.7204, Recall: 0.8487, F1-measure: 0.7793\n","Micro-average quality numbers\n","Precision: 0.7522, Recall: 0.8268, F1-measure: 0.7877\n","Micro-average quality numbers\n","Precision: 0.7656, Recall: 0.8127, F1-measure: 0.7884\n","Micro-average quality numbers\n","Precision: 0.7755, Recall: 0.8024, F1-measure: 0.7887\n","Micro-average quality numbers\n","Precision: 0.7856, Recall: 0.7908, F1-measure: 0.7882\n","Micro-average quality numbers\n","Precision: 0.7976, Recall: 0.7788, F1-measure: 0.7881\n","Micro-average quality numbers\n","Precision: 0.8066, Recall: 0.7602, F1-measure: 0.7827\n","Micro-average quality numbers\n","Precision: 0.8159, Recall: 0.7383, F1-measure: 0.7752\n","Micro-average quality numbers\n","Precision: 0.8303, Recall: 0.7040, F1-measure: 0.7620\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EX7DFjtdTOl3","executionInfo":{"status":"error","timestamp":1600933190170,"user_tz":-420,"elapsed":2541345,"user":{"displayName":"THAI NGUYEN QUOC","photoUrl":"","userId":"14553785917059170356"}},"outputId":"e1465be1-c0f5-45d4-8d96-070487674547","colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import numpy as np\n","import itertools\n","matplotlib.rcParams.update({'font.size': 16})\n","labels = [0,1]\n","cn = confusion_matrix(Y_test,y_pred,labels=labels)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-86e27e800912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'font.size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"]}]},{"cell_type":"code","metadata":{"id":"76NoIT0Yo0Hp","cellView":"form"},"source":["#@title plot confusion matrix\n","def plot_confusion_matrix(cm,\n","                          title='Confusion matrix',\n","                          cmap=None,\n","                          normalize=True,\n","                          target_names=None,\n","                          path_file='1.svg'):\n","    \n","    accuracy = np.trace(cm) / float(np.sum(cm))\n","    misclass = 1 - accuracy\n","\n","    if cmap is None:\n","        cmap = plt.get_cmap('Blues')\n","\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","\n","    if target_names is not None:\n","        tick_marks = np.arange(len(target_names))\n","        plt.xticks(tick_marks, target_names)\n","        plt.yticks(tick_marks, target_names)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","\n","    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        if normalize:\n","            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","        else:\n","            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.savefig(path_file,format='svg')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VoV9rJHCeZSy"},"source":["plot_confusion_matrix(cn,title='Data vreview',normalize=False,target_names=labels,path_file=path+'bert_rcnn_vreview_cm.svg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JyYN_Xe-5v4d"},"source":[""],"execution_count":null,"outputs":[]}]}